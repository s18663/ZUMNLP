# -*- coding: utf-8 -*-
"""Stage3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XYMOLt-o6TCxmdg9iArPe4oamxeEhdNA
"""

!wget https://raw.githubusercontent.com/MHDBST/PerSenT/main/train.csv



!pip install -U spacy
!python -m spacy download en_core_web_md

"""Importy"""

import collections
import pathlib
import pandas as pd
import re
import logging
import spacy
from tqdm import tqdm
tqdm.pandas()

import tensorflow as tf

from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow.keras import utils
from tensorflow.keras.layers import TextVectorization
import numpy as np
from keras.models import Sequential, Model

import tensorflow_datasets as tfds
import matplotlib.pyplot as plt

def plot_graphs(history, metric):
  plt.plot(history.history[metric])
  plt.plot(history.history['val_'+metric], '')
  plt.xlabel("Epochs")
  plt.ylabel(metric)
  plt.legend([metric, 'val_'+metric])

"""Wczytanie danych"""

df = pd.read_csv('train.csv')

data = df[['TRUE_SENTIMENT','DOCUMENT']]

data['TRUE_SENTIMENT'].replace('Negative',0,inplace=True)
data['TRUE_SENTIMENT'].replace('Neutral',1,inplace=True)
data['TRUE_SENTIMENT'].replace('Positive',2,inplace=True)

data['DOCUMENT'] = data['DOCUMENT'].apply(lambda x:x.lower())

character_re = re.compile(r"[^a-z\s']")
data['DOCUMENT'] = data['DOCUMENT'].apply(lambda x:character_re.sub('',x))

nlp = spacy.load(
    'en_core_web_md', disable=['ner','parser'])
nlp.add_pipe('sentencizer')

def remove_stopwords(text):
  return ' '.join(filter(lambda x: x not in nlp.Defaults.stop_words,text.split()))

data['DOCUMENT'].apply(remove_stopwords)

a = data[data['TRUE_SENTIMENT']==0]
b = data[data['TRUE_SENTIMENT']==1]
c = data[data['TRUE_SENTIMENT']==2]

#a.to_csv("Negative.csv",index=False)
#b.to_csv("Neutral.csv",index=False)
#c.to_csv("Positive.csv",index=False)

a.rename(columns={'TRUE_SENTIMENT':'label'}, inplace=True)
b.rename(columns={'TRUE_SENTIMENT':'label'}, inplace=True)
c.rename(columns={'TRUE_SENTIMENT':'label'}, inplace=True)

dataAll = pd.concat([a,b,c])

def lemmatize(text):
  return ' '.join([x.lemma_ for x in nlp(text)])

dataAll['DOCUMENT'] = dataAll['DOCUMENT'].progress_apply(lemmatize)

"""Keras dodatkowe importy"""

!pip install keras

from keras.models import load_model

from keras.layers import Embedding,Input,Dense,Flatten
from keras.initializers import Constant
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences, to_categorical

sents = [sent.split() for sent in dataAll['DOCUMENT']]

MAX_NUM_WORDS = 100
MAX_SEQUENCE_LENGTH = 20
tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)
tokenizer.fit_on_texts(sents)
sequences = tokenizer.texts_to_sequences(sents)

word_index = tokenizer.word_index
data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)

"""Glove setup"""

!wget  https://nlp.stanford.edu/data/glove.6B.zip

from zipfile import ZipFile
with ZipFile('glove.6B.zip', 'r') as z:
  z.extractall()

labels = to_categorical(dataAll['label'],num_classes=3)

print(labels)

print(data)

"""train_split, matrix"""

X_train,X_test,y_train,y_test = train_test_split(data,labels,test_size=0.1,stratify=labels,random_state=42)

embeddings_index = dict()
f = open('./glove.6B.100d.txt')
for line in f:
	values = line.split()
	word = values[0]
	coefs = np.asarray(values[1:], dtype='float32')
	embeddings_index[word] = coefs
f.close()

vocab = tokenizer.sequences_to_texts(data)
vocab_size = len(tokenizer.word_index) + 1 
print(vocab_size)

embedding_matrix = np.zeros((vocab_size, 100))
for word, i in tokenizer.word_index.items():
	embedding_vector = embeddings_index.get(word)
	if embedding_vector is not None:
		embedding_matrix[i] = embedding_vector

print(embedding_matrix)

"""model"""

model = tf.keras.Sequential([
    Input(shape=(MAX_SEQUENCE_LENGTH,),dtype='int32'),
    Embedding(
        input_dim=vocab_size,
        output_dim=100,
        weights=[embedding_matrix],
        input_length=MAX_SEQUENCE_LENGTH,
        trainable=True),
    Flatten(),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(3, activation='relu')
])

model.summary()

model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(1e-4),
              metrics=['accuracy'])

history = model.fit(X_train,y_train, epochs=20,
                    validation_data=(X_test,y_test),
                    validation_steps=30,
                    steps_per_epoch=20,
                    batch_size=32,
                    verbose=True
                    )

"""fine-tuning"""

print("Number of layers in the base model: ", len(model.layers))
fine_tune_at = 2
for layer in model.layers[:fine_tune_at]:
  layer.trainable = False

base_learning_rate = 0.0001

model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
              optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10),
              metrics=['accuracy'])

model.summary()

!sudo pip install h5py

es=tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1,patience=50)

mc = tf.keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True,verbose=1)

initial_epochs = 20
fine_tune_epochs = 10
total_epochs =  initial_epochs + fine_tune_epochs



history_fine = model.fit(X_train,y_train,
                         epochs=total_epochs,
                         initial_epoch=history.epoch[-1],
                         steps_per_epoch=20,
                          batch_size=32,
                          verbose=True,
                         validation_data=(X_test,y_test),
                         callbacks=[es,mc])

model = load_model('best_model.h5')

model.summary()

"""wykresy wynikowe"""

plot_graphs(history,'loss')
plot_graphs(history_fine,'loss')

plot_graphs(history,'accuracy')
plot_graphs(history_fine,'accuracy')