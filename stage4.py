# -*- coding: utf-8 -*-
"""Stage4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gIJYazAcY7OY0YGTUJa5FV2MGG4aLSzV
"""

!wget https://raw.githubusercontent.com/MHDBST/PerSenT/main/train.csv

!pip uninstall -y transformers accelerate
!pip install transformers accelerate
!pip install xformers

!pip install -U datasets
!pip install -U spacy
!python -m spacy download en_core_web_md

pip install --upgrade accelerate

import transformers
transformers.__version__
import pandas as pd
import spacy
import re
from tqdm import tqdm
tqdm.pandas()
import numpy as np

from datasets import Dataset

data = pd.read_csv('train.csv')
data = data[['TRUE_SENTIMENT','DOCUMENT']]
data.rename(columns={'TRUE_SENTIMENT':'label'}, inplace=True)

data['label'].replace('Negative',0,inplace=True)
data['label'].replace('Neutral',1,inplace=True)
data['label'].replace('Positive',1,inplace=True)

a = data[data['label']==0]
c = data[data['label']==1]

dataAll = pd.concat([a,c])

dataAll.sample(5)

class SimpleDataset:
    def __init__(self, tokenized_texts):
        self.tokenized_texts = tokenized_texts
    
    def __len__(self):
        return len(self.tokenized_texts["input_ids"])
    
    def __getitem__(self, idx):
        return {k: v[idx] for k, v in self.tokenized_texts.items()}

from transformers import AutoTokenizer, AutoModelForSequenceClassification,pipeline,Trainer
import torch


sentiment_analysis = pipeline("sentiment-analysis",model="siebert/sentiment-roberta-large-english")
siebert = AutoModelForSequenceClassification.from_pretrained("siebert/sentiment-roberta-large-english",num_labels=2)
tokenizer = AutoTokenizer.from_pretrained("siebert/sentiment-roberta-large-english")
trainer = Trainer(model=siebert)

tokenized_texts = tokenizer(dataAll['DOCUMENT'].tolist(),truncation=True,padding=True)
pred_dataset = SimpleDataset(tokenized_texts)

predictions = trainer.predict(pred_dataset)

preds = predictions.predictions.argmax(-1)
labels = pd.Series(preds).map(siebert.config.id2label)
scores = (np.exp(predictions[0])/np.exp(predictions[0]).sum(-1,keepdims=True)).max(1)

df = pd.DataFrame(list(zip(dataAll['DOCUMENTS'],preds,labels,scores)), columns=['text','pred','label','score'])
df.head()